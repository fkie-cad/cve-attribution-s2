import datetime
import gzip
import logging
import re
from enum import Enum

import dateutil.parser
import requests
from typing import Tuple, List, Dict
import urllib.parse
from lxml import html
import json
import hashlib

import util

logger = logging.getLogger(__name__.upper())


class SourceType(Enum):
    UNKNOWN = 0
    ARCHIVE = 1
    LIVE = 2
    IGNORE = 3

    @staticmethod
    def guess(name: str):
        if SourceType.probably_archive(name):
            return SourceType.ARCHIVE
        if SourceType.probably_useless(name):
            return SourceType.IGNORE
        if SourceType.probably_live(name):
            return SourceType.LIVE
        return SourceType.UNKNOWN

    @staticmethod
    def probably_archive(name: str) -> bool:
        return re.search(r'-\d{4}$', name) is not None

    @staticmethod
    def probably_useless(name: str) -> bool:
        # the 'CVE-Recent' feed only contains new CVEs, we don't need that one
        return 'recent' in name.lower()

    @staticmethod
    def probably_live(name: str) -> bool:
        # the 'CVE-Modified' feed contains new and modified CVEs of the past 8 days.
        return 'modified' in name.lower()


class Source:
    def __init__(self, http_session: requests.Session):
        self.name = ''
        self.meta_url = ''
        self.data_url = ''
        self._http = http_session
        self._meta = {}
        self._data = {}

    @property
    def meta(self) -> Dict:
        return self._meta.copy()

    @property
    def data(self) -> Dict:
        return self._data

    @property
    def type(self):
        return SourceType.guess(self.name)

    def pull_meta(self):
        logger.info(f'Pulling meta from \'{self.meta_url}\' ({self.name})')

        response = self._http.get(self.meta_url)
        meta = Source._raw_meta_to_dict(response.content)
        self._meta = Source._meta_dict_convert(meta)

        logger.debug(f'Got meta:\n{json.dumps(self._meta, indent=4, cls=util.BetterEncoder)}')

    def pull_data(self):
        logger.info(f'Pulling data from \'{self.data_url}\' ({self.name})')

        response = self._http.get(self.data_url)
        compressed = response.content
        decompressed = gzip.decompress(compressed)
        digest = hashlib.sha256(decompressed).hexdigest().upper()
        self._data = json.loads(decompressed)

        pretty_data = json.dumps(
            {"size": len(decompressed), "gzSize": len(compressed), "sha256": digest},
            indent=4,
        )
        logger.debug(f'Got data:\n{pretty_data}')

    def flush_data(self):
        logger.info(f'Pulling data from \'{self.data_url}\' ({self.name})')
        del self._data
        self._data = {}

    def __str__(self) -> str:
        return json.dumps(
            {'name': self.name, 'type': self.type.name, 'metaUrl': self.meta_url, 'dataUrl': self.data_url},
            indent=4
        )

    @staticmethod
    def _meta_dict_convert(meta: Dict):
        m = meta.copy()
        m['lastModifiedDate'] = dateutil.parser.isoparse(meta['lastModifiedDate'])
        m['size'] = int(meta['size'])
        m['zipSize'] = int(meta['zipSize'])
        m['gzSize'] = int(meta['gzSize'])
        return m

    @staticmethod
    def _raw_meta_to_dict(raw_meta: bytes) -> Dict:
        decoded = raw_meta.decode('utf-8')

        meta = {}

        for line in decoded.split('\n'):
            if len(line) == 0:
                continue
            key, value = line.strip().split(':', maxsplit=1)
            meta[key] = value

        if meta.keys() < {'lastModifiedDate', 'size', 'zipSize', 'gzSize', 'sha256'}:
            raise ValueError

        return meta


class SourceList(list):
    def pull_meta(self):
        for src in self:
            src.pull_meta()

    def pull_data(self):
        for src in self:
            src.pull_data()

    def flush_data(self):
        for src in self:
            src.pull_data()


class SourceAggregator:

    def __init__(self, root: str):
        self._feed_root = urllib.parse.urlparse(root)
        self._base = urllib.parse.urlparse(f'{self._feed_root.scheme}://{self._feed_root.netloc}')
        self._http = requests.Session()
        self._sources = {}

    def update_sources(self):
        logger.info(f'Pulling source list from {self._base.geturl()}')

        response = self._http.get(self._feed_root.geturl())

        doc = html.fromstring(response.content)

        names = XPath.names(doc)
        metas = XPath.meta(doc)
        data = XPath.json(doc)

        for name, meta, data in zip(names, metas, data):
            if name not in self._sources:
                self._sources[name] = Source(http_session=self._http)
            self._sources[name].meta_url = self._absolute_url(meta)
            self._sources[name].data_url = self._absolute_url(data)
            self._sources[name].name = name

            logger.debug(f'Got source:\n{self._sources[name]}')
        logger.info(f'Pulled {len(self._sources)} sources (L:{len(self.live_sources)}/A:{len(self.archive_sources)}/'
                    f'I:{len(self.ignored_sources)})')

    @property
    def sources(self) -> SourceList[Source]:
        if len(self._sources) == 0:
            self.update_sources()
        return SourceList(self._sources.values())

    @property
    def live_sources(self) -> SourceList[Source]:
        live = SourceList()
        for src in self.sources:
            if src.type != SourceType.LIVE:
                continue
            live.append(src)
        return live

    @property
    def ignored_sources(self) -> SourceList[Source]:
        ignored = SourceList()
        for src in self.sources:
            if src.type != SourceType.IGNORE:
                continue
            ignored.append(src)
        return ignored

    @property
    def archive_sources(self) -> SourceList[Source]:
        archive = SourceList()
        for src in self.sources:
            if src.type != SourceType.ARCHIVE:
                continue
            archive.append(src)
        return archive

    def _absolute_url(self, relative: str) -> str:
        return urllib.parse.urljoin(self._base.geturl(), relative)

    def close(self):
        self._http.close()


class XPath:
    _BASE_ = './/div[contains(@id, "vuln-feed-table")]//table[contains(@class, "xml-feed-table")]//'
    NAME = _BASE_ + 'td[contains(text(), "CVE-")]/text()'
    META = _BASE_ + 'a[contains(@href, ".meta")]/@href'
    JSON_GZ = _BASE_ + 'a[contains(@href, ".json.gz")]/@href'

    @staticmethod
    def names(document: html.HtmlElement) -> List[str]:
        return document.xpath(XPath.NAME)

    @staticmethod
    def meta(document: html.HtmlElement) -> List[str]:
        return document.xpath(XPath.META)

    @staticmethod
    def json(document: html.HtmlElement) -> List[str]:
        return document.xpath(XPath.JSON_GZ)
